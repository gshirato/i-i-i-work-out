# 目次

- パーセプトロン
- ニューラルネットワーク
- ネットワークの学習
- 誤差逆伝播法
- 学習に関するテクニック
- 畳み込みニューラルネットワーク
- ディープラーニング

---

# 1. パーセプトロン

+++

## パーセプトロンとは

Blah x3

+++

## パーセプトロンの動作原理

入力信号$x_1, x_2$, 出力信号$y$, 重み$w_1, w_2$について\
ニューロンの発火閾値$\theta$

`\[
y = 
\begin{cases}
    0 \ \left( w_1x_1 + w_2x_2 \leq \theta \right) \\
    1 \ \left( w_1x_1 + w_2x_2 >    \theta \right)
\end{cases}
\]`

+++

### 重みについて

> 重みは、電流で言うところの「抵抗」に相当します。抵抗は電流の流れにくさを 決めるパラメータであり、抵抗が低ければ低いほど大きな電流が流れます。一 方、パーセプトロンの重みは、その値が大きければ大きいほど、大きな信号が 流れることを意味します。抵抗も重みも信号の流れにくさ(流れやすさ)をコ ントロールするという点では同じ働きをします。
(本書き写し)

+++

## AND ゲート

|$x_1$|$x_2$|$y$|
|:----|:----|--:|
|  0  |  0  | 0 |
|  1  |  0  | 0 |
|  0  |  1  | 0 |
|  1  |  1  | 1 |

これをパーセプトロンで表す<br> 
-> 真理値表を満たすように($w_1$,$w_2$,$\theta$)を決める <br>
-> e.g.) (0.5, 0.5, 0.7), (0.5, 0.5, 0.8)

+++

## NAND

|$x_1$|$x_2$|$y$|
|:----|:----|--:|
|  0  |  0  | 1 |
|  1  |  0  | 1 |
|  0  |  1  | 1 |
|  1  |  1  | 0 |

ANDの符号変換だけで良い<br>
-> e.g.) (-0.5, -0.5, -0.7)

---

## 1.2 パーセプトロンの実装

``` python
def AND(x1, x2, w1=1, w2=1, theta=1):
    theta = 1, 1, 1
    y = x1*w1 + x2*w2
    return int(y<=theta)
```
+++

### 式変換（閾値からバイアスへ）

$\theta \rightarrow -b$して移項

`\[
y = 
\begin{cases}
    0 \ \left( b + w_1x_1 + w_2x_2 \leq 0 \right) \\
    1 \ \left( b + w_1x_1 + w_2x_2 >    0 \right)
\end{cases}
\]`

+++

### コード

``` python
def AND(x1, x2, w1=1, w2=1, b=-1):
    
    x = np.array([x1, x2])
    w = np.array([w1, w2])
    
    y = np.sum(w*x) + b
    return int(y>0)

```
+++

### NAND
``` python
# 重みとバイアスのみ異なる！
def NAND(x1, x2, w1=-1, w2=-1, b=1):
    
    x = np.array([x1, x2])
    w = np.array([w1, w2])
    y = np.sum(w*x) + b
    return int(y>0)
    
def OR(x1, x2, w1=0.5, w2=0.5, b=-0.2):
    
    x = np.array([x1, x2])
    w = np.array([w1, w2])
    y = np.sum(w*x) + b
    return int(y>0)
```
下位関数にする？
